{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Mini challenge retrieval augmented generation (RAG)](#toc0_)\n",
    "> Oliver Pejic; Arian Iseni; Christof Weickhardt. \n",
    "\n",
    "> Here Summary\n",
    "\n",
    "- [Description of the task](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/exercises/20240911_NPR_Trainingscenter_MiniChallenge_RAG-1729000458.pdf)\n",
    "- [Introduction RAG](https://spaces.technik.fhnw.ch/storage/uploads/spaces/81/Retrieval-Augmented-Generation-Intro-1727189074.pdf)\n",
    "\n",
    "[**Table of content**](#toc0_)\n",
    "- [Mini challenge retrieval augmented generation (RAG)](#toc1_)\n",
    "- [Our approach](#toc2_)\n",
    "- [Setup](#toc3_)\n",
    "- [Data Loading & Preprocessing](#toc4_) \n",
    "- [Chunking](#toc5_)\n",
    "- [Embedding & VectorDB](#toc6_)\n",
    "- [Reasoning for our models](#toc7_)\n",
    "- [Baseline Pipeline](#toc8_)\n",
    "- [Evaluation Metrix](#toc9_)\n",
    "  - [Ragas Metrics](#toc9_1_)\n",
    "  - [Non-LLM Based Metrics](#toc9_2_)\n",
    "- [Evaluation Data](#toc10_)\n",
    "- [Experiment 1:](#toc11_)\n",
    "- [Experiment 2:](#toc12_)\n",
    "- [Experiment 3:](#toc13_)\n",
    "- [Experiment 4:](#toc14_)\n",
    "- [Personal Takeaways](#toc15_)\n",
    "- [AI Tools - Improving Our Project with Assistance](#toc16_)\n",
    "  - [How AI Tools Help Us](#toc16_1_)\n",
    "  - [Using ChatGPT](#toc16_2_)\n",
    "  - [Using GitHub Copilot](#toc16_3_)\n",
    "  - [What Works Best When Asking AI for Help](#toc16_4_)\n",
    "  - [Summary](#toc16_5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Setup](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Data Loading & Preprocessing](#toc0_)\n",
    "\n",
    "We will now load the data into a pandas DataFrame and preprocess it. This preprocessing is informed by the insights documented in the `notebooks/cleaning.ipynb` notebook.\n",
    "\n",
    "The preprocessing steps are implemented in the `src/preprocess.py` file. A class named `TextPreprocessor` is responsible for preparing the main dataset for indexing and retrieval. The preprocessing includes the following steps:\n",
    "\n",
    "The preprocessing includes the following steps:\n",
    "\n",
    "1.\tLanguage Detection and Filtering: Retain only English texts to ensure language consistency.\n",
    "2.\tHTML Cleaning: Strip out HTML tags to focus on the raw textual data.\n",
    "3.\tSpecial Character Removal: Remove unwanted non-alphanumeric characters while retaining essential punctuation.\n",
    "4.\tDuplicate Removal: Eliminate duplicate text entries to ensure data uniqueness.\n",
    "5.\tUnique Identifier Generation: Create a unique ID for each dataset row using a hash of its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/raw/cleantech_media_dataset_v2_2024-02-23.csv\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete author col\n",
    "df = df.drop(columns=['author'])\n",
    "# rename Unnamed: 0 to 'id'\n",
    "df = df.rename(columns={'Unnamed: 0': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocess import TextPreprocessor\n",
    "\n",
    "tp = TextPreprocessor(df, 'content')\n",
    "\n",
    "cleaned_data = tp.preprocess_data()\n",
    "\n",
    "tp.add_unique_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessed data is then saved to a new CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('../data/preprocessed'):\n",
    "    cleaned_data.to_parquet('data/preprocessed/clean_cleantech.parquet')\n",
    "else:\n",
    "    os.makedirs('../data/preprocessed')\n",
    "    cleaned_data.to_parquet('data/preprocessed/clean_cleantech.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Chunking](#toc0_)\n",
    "Text chunking involves dividing large documents into smaller, manageable pieces or “chunks” that are easier to process and index. This is particularly useful when dealing with extensive datasets or documents, where processing the entire content at once would be computationally expensive and inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "def create_documents(df: pd.DataFrame, text_splitter, verbose=True):\n",
    "    metadata_cols = ['url', 'domain', 'title', 'date', 'id']\n",
    "    if not all(col in df.columns for col in metadata_cols + ['content']):\n",
    "        raise ValueError(\n",
    "            f\"DataFrame must contain all metadata columns and a 'content' column: {metadata_cols + ['content']}\")\n",
    "\n",
    "    metadata = df[metadata_cols].rename(columns={'id': 'origin_doc_id'}).to_dict('records')\n",
    "    for i, m in enumerate(metadata):\n",
    "        metadata[i] = {k: 'None' if v is None else v for k, v in m.items()}\n",
    "\n",
    "    docs = text_splitter.create_documents(df['content'], metadata)\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"{text_splitter.__class__.__name__}: \"\n",
    "            f\"Number of documents created: {len(docs)}, \"\n",
    "            f\"Number of rows in source df: {len(df)}, \"\n",
    "            f\"Percentage of documents created: {len(docs) / len(df) * 100:.2f}%\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "documents = create_documents(df, recursive_text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part splits the DataFrame containing large texts into smaller, non-overlapping chunks for easier processing. Each chunk is up to 1000 characters long. The RecursiveCharacterTextSplitter is initialized with no overlap to ensure each character in the original text is unique to one chunk, preventing redundancy and maintaining clear separation between chunks. The function checks for necessary metadata columns, handles missing values, and uses these metadata along with the text content to generate smaller document chunks. It provides feedback on the number of documents created compared to the original DataFrame size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc6_'></a>[Embedding & VectorDB](#toc0_)\n",
    "In a RAG system, both embeddings and a vector database play crucial roles in the process of augmenting language generation with retrieval capabilities. Embeddings and vector databases in RAG systems bridge the gap between raw user queries and informative content in vast document collections, enhancing the generation of responses by making them more relevant and contextually aware. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.custom_embeddings import bge_m3_embed, qwen2_embed, nomic_embed\n",
    "\n",
    "# Initialize a list with all three embedding models.\n",
    "embedding_models = [bge_m3_embed, qwen2_embed, nomic_embed]\n",
    "\n",
    "# Loop through each model, print its name, embed a sample query, and print the first 20 dimensions of the resulting embedding.\n",
    "for model in embedding_models:\n",
    "    print(model.model_name)\n",
    "    embedding = model.embed_query(\"The company is also aiming to reduce gas flaring?\")\n",
    "    print(embedding[:20])\n",
    "    print()\n",
    "\n",
    "# Define a function to create unique collection names based on the model and text splitter.\n",
    "from src.vectorstorage import EmbeddingVectorStorage\n",
    "def get_col_name_vectordb(embeddings, text_splitter):\n",
    "    return f\"{embeddings.model_name}_{text_splitter.__class__.__name__}\"\n",
    "\n",
    "# Create a dictionary to store vector storage instances.\n",
    "vector_stores = {}\n",
    "\n",
    "# For each embedding model, create a vector storage instance and include documents.\n",
    "for model in embedding_models:\n",
    "    collection_name = get_col_name_vectordb(model, recursive_text_splitter)\n",
    "    print(f\"Collection name: {collection_name}\")\n",
    "    vector_storage = EmbeddingVectorStorage(method_of_embedding=model, collection=collection_name)\n",
    "    vector_storage.include_documents(documents, should_verbose=True)\n",
    "    vector_stores[model.model_name] = vector_storage\n",
    "\n",
    "# Print the dictionary of vector storages.\n",
    "print(vector_stores)\n",
    "\n",
    "# Query each vector storage for documents similar to a specific query and print the results.\n",
    "query = \"The company is also aiming to reduce gas flaring?\"\n",
    "for model_name, vector_store in vector_stores.items():\n",
    "    print(f\"Results for model: {model_name}\")\n",
    "    try:\n",
    "        results = vector_store.search_similar_w_scores(query)\n",
    "        for doc, score in results:\n",
    "            print(f\"Document: {doc}\")\n",
    "            print(f\"Score: {score}\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching in vector store '{model_name}': {e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initializes and utilizes custom embedding models along with a vector storage system to handle and query large sets of document embeddings efficiently. The custom embedding models are defined in the `CustomHuggingFaceEndpointEmbeddings` class, which extends the capabilities of HuggingFace's endpoint embeddings to include a `model_name` attribute. This addition helps identify and manage multiple models within our system.\n",
    "\n",
    "Each model is configured with a specific server endpoint URL and is responsible for transforming text into embeddings. These embeddings are then stored and managed in a `chromadb` based vector storage system, which allows for efficient retrieval and management of the vector data.\n",
    "\n",
    "The script performs the following key operations:\n",
    "1. Initializes three embedding models with unique names and endpoints.\n",
    "2. Embeds a sample query using each model and prints the first 20 elements of the embeddings for verification.\n",
    "3. Defines a function to generate unique collection names for storing embeddings based on the model's name and the text splitter class, ensuring organized data management.\n",
    "4. Creates a vector storage instance for each model and includes the processed documents into the database, with progress updates provided if verbose is enabled.\n",
    "5. Demonstrates querying the vector storage with a sample text to find and print documents similar to the query along with their similarity scores.\n",
    "6. Includes error handling to manage and report potential issues during the search operations, ensuring the robustness of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc7_'></a>[Reasoning for our models](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc8_'></a>[Baseline Pipeline](#toc0_)\n",
    "Our baseline pipeline for question answering employs a multi-component approach integrating embeddings, language models, and various utilities from the LangChain library. This setup allows for efficient retrieval and processing of relevant documents to generate accurate answers to user queries.\n",
    "\n",
    "**Components**\n",
    "\n",
    "1.\tEmbeddings:\n",
    "- We utilize the bge_m3_vectordb from the LangChain library, which is backed by embeddings from Hugging Face. These embeddings are crucial for retrieving the most relevant documents from our database based on the semantic similarity to the input query.\n",
    "2.\tLanguage Models:\n",
    "- The core of our language processing is handled by the OllamaLLM from the LangChain’s Ollama module, specifically using the qwen2.5:0.5b-instruct-q4_0 model. This model is designed to understand and generate human-like text based on the context provided by the retrieved documents.\n",
    "3.\tRetrieval and Processing Utilities:\n",
    "- Retriever: The basic retriever setup uses the as_retriever() method from bge_m3_vectordb, which efficiently identifies and fetches documents relevant to the input question.\n",
    "- [**Prompt**](https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=2d6cd9b7-5b49-44db-a523-a13c23f12f29): We leverage a pre-built prompt from LangChain’s hub, specifically rlm/rag-prompt. This prompt is tailored to guide the model in generating coherent and contextually appropriate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from src.custom_embeddings import bge_m3_vectordb\n",
    "from langchain import hub\n",
    "\n",
    "\n",
    "qwen2_5_0_5b_model = 'qwen2.5:0.5b-instruct-q4_0'\n",
    "\n",
    "\n",
    "basic_retriever = bge_m3_vectordb.as_retriever()\n",
    "llm_model = OllamaLLM(model=qwen2_5_0_5b_model)\n",
    "basic_prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Pipeline Execution**](https://python.langchain.com/v0.1/docs/use_cases/question_answering/sources/)\n",
    "\n",
    "The execution of the pipeline is initialized by invoking the basic_rag_chain with a sample question. The process includes:\n",
    "\n",
    "- Retrieving contextually relevant documents using the embedded database.\n",
    "- Formatting the retrieved documents into a structured format that is then processed by the rag_chain_from_docs sequence.\n",
    "- Generating an answer through the orchestrated interaction of the retriever, prompt, and language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | basic_prompt\n",
    "    | llm_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "basic_rag_chain = RunnableParallel(\n",
    "    {\"context\": basic_retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `basic_rag_chain` can now generate answers for questions. Here you can see a test with a question to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_rag_chain.invoke(\"The company is also aiming to reduce gas flaring?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Evaluation Metrix](#toc0_)\n",
    "\n",
    "To properly assess and compare different computational systems, we use a structured evaluation approach. This includes using the `ragas` library for language model-based metrics and traditional metrics for other types of systems. Below, we explain the specific metrics we use in our evaluations, designed to accurately gauge various aspects of system performance.\n",
    "\n",
    "## <a id='toc9_1_'></a>[Ragas Metrics](#toc0_)\n",
    "\n",
    "We use the following metrics from the ragas library to evaluate the quality of answers and how relevant the contexts are that language model-based systems provide:\n",
    "\n",
    "1.\t[**Faithfulness**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/faithfulness.html): Measures the factual consistency of the generated answer against the given context. It is computed by identifying claims in the answer and verifying if each can be inferred from the context. The metric ranges from 0 to 1, where higher scores indicate better factual consistency.\n",
    "\n",
    "2.\t[**Answer Relevancy**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_relevance.html): Assesses the pertinence of the generated answer to the given prompt. It is calculated by comparing the mean cosine similarity of the original question to artificially generated questions based on the answer. High scores are given to answers that address the prompt directly and appropriately, without redundant details or omissions.\n",
    "\n",
    "3.\t[**Context Precision**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/context_precision.html): Evaluates the accuracy with which relevant items from the context are retrieved and ranked. This metric checks if the essential chunks of context appear at the top of the ranking, using a range from 0 to 1 where higher values indicate better precision.\n",
    "\n",
    "4.\t[**Context Entity Recall**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/context_entities_recall.html): Measures the recall of entities from the retrieved context compared to the entities present in the ground truth. This metric is crucial for use cases where specific entity-related information is necessary, such as historical QA or tourism help desks, indicating the fraction of correctly recalled entities.\n",
    "\n",
    "5.\t[**Answer Similarity**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/semantic_similarity.html): Assesses the semantic resemblance between the generated answer and the ideal answer (ground truth). This metric uses a cross-encoder model to calculate semantic similarity, with values ranging from 0 to 1, where higher scores denote a closer alignment with the ground truth.\n",
    "\n",
    "6.\t[**Answer Correctness**](https://docs.ragas.io/en/v0.1.21/concepts/metrics/answer_correctness.html): Evaluates both the semantic and factual accuracy of the generated answer in relation to the ground truth. The metric combines aspects of semantic and factual similarity using a weighted scheme and offers a scoring range from 0 to 1. Higher scores indicate better alignment and correctness, with an optional ‘threshold’ for rounding scores to binary values if needed.\n",
    "\n",
    "## <a id='toc9_2_'></a>[Non-LLM Based Metrics](#toc0_)\n",
    "\n",
    "To assess traditional retrieval systems, we use the following non-LLM based metrics, which are essential for evaluating how well these systems perform and how relevant their results are:\n",
    "\n",
    "1.\t**Mean Reciprocal Rank (MRR)**: A ranking quality metric that evaluates how quickly a system can present the first relevant item among its results. MRR is calculated as the average of the reciprocal ranks of the first relevant answer for different queries, where the reciprocal rank is the inverse of the rank at which the first relevant item appears. MRR values range from 0 to 1, with higher values indicating that the first relevant item typically appears earlier in the list, thus suggesting better performance.\n",
    "\n",
    "2.\t**Precision at K**: Measures the accuracy of the system in identifying relevant items within the top K results. Precision at K is the proportion of relevant items among the top K positions in the list, highlighting the system’s effectiveness at ranking relevant documents higher. This metric helps to understand how many of the top K items are actually relevant to the user, with values ranging from 0 to 1 where higher values indicate better accuracy.\n",
    "\n",
    "3.\t**Recall at K**: Evaluates how comprehensive the system’s retrieval is by measuring the proportion of relevant items that are retrieved within the top K results out of all relevant items available in the dataset. This metric assesses the system’s ability to include as many relevant items as possible within the top ranks, reflecting its effectiveness in covering the relevant documents needed for user queries. Similar to Precision, Recall values range from 0 to 1, with higher values indicating more comprehensive retrieval of relevant items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc10'></a>[Evaluation Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv('data/eval_dataset/cleantech_rag_evaluation_data_2024-09-20.csv')\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Experiment 1:](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Experiment 2:](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc13_'></a>[Experiment 3:](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc14_'></a>[Experiment 4:](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc15_'></a>[Personal Takeaways](#toc0_)\n",
    "\n",
    "Though this work is to proof understanding and skills surrounding the task at hand, we would like to take a few minutes and each reflect in a few sentences the journey throught this mini-challenge.\n",
    "\n",
    "### Arian\n",
    "\n",
    "In this challenge, I had the incredible opportunity to learn so much about large language models, how a retrieval system works, and how the metrics are used to rank the relevant information of a query. Llm as a judge was also an amazing experience, seeing how the models judge each other. I found functionalities like Ollama or models from HuggingFace to be incredibly useful and easy to use. On top of all that, I also learned so much about Docker.\n",
    "\n",
    "### Oliver\n",
    "\n",
    "This project was an incredible opportunity for me to dive deep into the fascinating world of large language models, RAG systems, and how to use different models with HuggingFace, including Ollama. These tools are truly cutting-edge, combining advanced technology and data management to create smart responses. It was an amazing experience that has greatly increased my understanding and appreciation of how these powerful systems work.\n",
    "\n",
    "### Christof\n",
    "\n",
    "I've had a truly enriching experience taking part in this challenge! It's been an amazing opportunity to explore the cutting-edge RAG technologies that are changing the world. I've gained so much knowledge from new methodologies, scholarly papers and insights into infrastructure and computational frameworks. And it's been so inspiring to work with other like-minded individuals, sharing knowledge and skills, and fostering a real sense of camaraderie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc16_'></a>[AI Tools - Improving Our Project with Assistance](#toc0_)\n",
    "\n",
    "We've started using AI tools like ChatGPT and GitHub Copilot in our projects, and they've really helped us work better and solve problems faster. This document explains how we use these tools, what tasks they help with, and which ways of using them work best.\n",
    "\n",
    "## <a id='toc16_1_'></a>[How AI Tools Help Us](#toc0_)\n",
    "\n",
    "These AI tools make it easier for us to handle coding tasks and fix errors, freeing up time to focus on more important parts of our projects.\n",
    "\n",
    "### <a id='toc16_2_'></a>[Using ChatGPT](#toc0_)\n",
    "\n",
    "We use ChatGPT mainly for two things: fixing code errors and coming up with new ideas. When we run into a coding error, we paste the wrong code and the error message into ChatGPT, and it often gives us a solution right away. It's also great for brainstorming new ways to improve our projects, offering suggestions we might not think of on our own.\n",
    "\n",
    "### <a id='toc16_3_'></a>[Using GitHub Copilot](#toc0_)\n",
    "\n",
    "GitHub Copilot helps us write code faster. It's like having a coding assistant that suggests lines of code as we type, which is really helpful for straightforward tasks. For more complex problems, though, we still need to do a lot of the work ourselves.\n",
    "\n",
    "### <a id='toc16_4_'></a>[What Works Best When Asking AI for Help](#toc0_)\n",
    "\n",
    "Getting the best out of these AI tools depends on how we ask for help. For ChatGPT, being clear about what the error is and what part of the code isn't working is crucial. For new ideas, explaining exactly what we need helps the AI give us useful suggestions.\n",
    "\n",
    "For GitHub Copilot, it helps to start by clearly writing out what we want the code to do. This makes the tool more likely to suggest the right kind of code.\n",
    "\n",
    "### <a id='toc16_5_'></a>[Summary](#toc0_)\n",
    "\n",
    "Using AI tools like ChatGPT and GitHub Copilot has made our projects run smoother and has sped up how quickly we can write code and fix problems. ChatGPT is excellent for quickly dealing with errors and for helping us think of new ideas. GitHub Copilot is great for speeding up our coding, especially for simpler tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
