{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T22:38:53.573157Z",
     "start_time": "2024-11-05T22:38:53.568270Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "pandarallel.initialize(progress_bar=True, verbose=0)\n",
    "tqdm.pandas()\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "with open('secrets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('openai'):\n",
    "            secret = line.split('=')[1].strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = secret\n",
    "\n",
    "storage_path = './data/chromadb'"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:38:54.708258Z",
     "start_time": "2024-11-05T22:38:54.601329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('secrets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if line.startswith('api_token'):\n",
    "            token = line.split('=')[1].strip()\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model='http://100.67.185.22:8080',\n",
    "    huggingfacehub_api_token=token\n",
    ")\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type='standard_deviation'\n",
    ")"
   ],
   "id": "177fed536c391d76",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:39:01.055752Z",
     "start_time": "2024-11-05T22:39:01.053266Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # split the text into chunks\n",
    "def split_text(documents: list[Document]):\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    return chunks"
   ],
   "id": "ba7198a0cb5ed3a4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:39:01.289380Z",
     "start_time": "2024-11-05T22:39:01.286231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flatten, pad/truncate, and convert each embedding to a consistent 1D np.float32 array\n",
    "def prepare_embedding_for_chromadb(embedding):\n",
    "    # Flatten the embedding if it's nested\n",
    "    flat_embedding = [float(val) for sublist in embedding for val in sublist] if isinstance(embedding[0], (list, np.ndarray)) else embedding\n",
    "    \n",
    "    # Ensure the embedding is exactly 2048 dimensions\n",
    "    if len(flat_embedding) < 2048:\n",
    "        flat_embedding.extend([0.0] * (2048 - len(flat_embedding)))  # Pad with zeros if too short\n",
    "    elif len(flat_embedding) > 2048:\n",
    "        flat_embedding = flat_embedding[:2048]  # Truncate if too long\n",
    "    \n",
    "    # Convert to np.float32\n",
    "    return np.array(flat_embedding, dtype=np.float32)"
   ],
   "id": "dba71e5c16f5d705",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:41:04.644903Z",
     "start_time": "2024-11-05T22:41:01.586924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_m3 import EmbeddingModelM3\n",
    "\n",
    "embed_local = EmbeddingModelM3()"
   ],
   "id": "fc13248f96ba19c5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:41:08.274966Z",
     "start_time": "2024-11-05T22:41:06.289650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query_result = embed_local.embed_query(\"Hello, world!\") # local\n",
    "#query_result = embeddings.embed_query(\"Hello, world!\") # remote\n",
    "query_result[:3]"
   ],
   "id": "840feb7d2339660a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.026738807559013367, 0.42828133702278137, -0.6886834502220154]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chunking with Semantic Chunker from langchain\n",
    "### Breakpoint: Standard Deviation"
   ],
   "id": "df839abd6bf257ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:50:01.966259Z",
     "start_time": "2024-11-05T22:50:01.598580Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_parquet(\"data/clean_cleantech.parquet\")",
   "id": "555aebd342d637ca",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.content[0][:100]",
   "id": "e6ee31bf918a09a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['chunks'] = df['content'].parallel_apply(lambda content: split_text([Document(content)]))",
   "id": "af64f849d8c0de1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(3)",
   "id": "ef263715a1b4fe3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['chunk_size'] = df['chunks'].progress_apply(len)",
   "id": "426feab65131fd1f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(3)",
   "id": "d50a821060768953",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df['chunks'] = df['chunks'].progress_apply(lambda x: [t.page_content for t in x])",
   "id": "ce7585d47e357191",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.sample(5)",
   "id": "a9a5eefad6b8d205",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.to_parquet('data/processed/chunked_sd.parquet')",
   "id": "b21eb273c2794fc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_chunked = pd.read_parquet('data/processed/chunked_sd.parquet')",
   "id": "78e471fd626eca82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove empty chunks\n",
    "df_chunked['chunks'] = df_chunked['chunks'].progress_apply(lambda x: [y for y in x if len(y) > 0])"
   ],
   "id": "44c03b2a5ed27b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Embed the Chunks\n",
    "### model: BAAI/bge-m3"
   ],
   "id": "1fb9b55946e5343c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# embed the chunks\n",
    "df_chunked['embeddings'] = df_chunked['chunks'].parallel_apply(embeddings.embed_documents)"
   ],
   "id": "c6853c4bf5f52da3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_chunked.head(3)",
   "id": "f02eb8d52267756b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save the chunked and embedded data\n",
    "df_chunked.to_parquet('data/processed/chunked_sd_embedded.parquet')"
   ],
   "id": "3b33a97b2e70db89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Setting up the ChromaDB\n",
    "preparing the embedded parquet fiel for ChromaDB"
   ],
   "id": "e3fd17512b7c5e02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_parquet('data/processed/chunked_sd_embedded.parquet')",
   "id": "b2d739d77d91a762",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(3)",
   "id": "13035ac396307334",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.embeddings[0]",
   "id": "4a0b53a5da26f654",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "type(df.embeddings[0]), type(df.embeddings[0][0])",
   "id": "a27b7102af3d3f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### preparing the data for ChromaDB",
   "id": "ce53f73160ac02d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply the function to prepare embeddings\n",
    "tqdm.pandas()\n",
    "df['embeddings'] = df['embeddings'].progress_apply(prepare_embedding_for_chromadb)\n",
    "\n",
    "# Check the result\n",
    "print(\"Sample embedding type and shape:\", type(df['embeddings'][0]), df['embeddings'][0].shape, df['embeddings'][0].dtype)"
   ],
   "id": "4029e2b051038cbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.embeddings[0]",
   "id": "46faaa412d97e602",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert 'date' column to string format\n",
    "df['date'] = df['date'].astype(str)"
   ],
   "id": "2353667ac76e2695",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure all doc_ids are unique by adding a suffix to duplicates\n",
    "df['doc_id'] = df['doc_id'].astype(str)  # Ensure IDs are strings\n",
    "df['doc_id'] = df.groupby('doc_id').cumcount().astype(str) + '_' + df['doc_id']"
   ],
   "id": "11c34e1f2ebbeb9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### saving",
   "id": "c551a858551dec10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Specify the storage path\n",
    "settings = Settings()\n",
    "\n",
    "# Initialize ChromaDB client with persistent settings\n",
    "client = chromadb.PersistentClient(path=storage_path, settings=settings)\n",
    "collection_name = \"energy_articles\"\n",
    "\n",
    "# Delete and recreate collection\n",
    "if collection_name in [col.name for col in client.list_collections()]:\n",
    "    client.delete_collection(collection_name)\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "df['embeddings'] = df['embeddings'].progress_apply(lambda x: x.tolist() if isinstance(x, np.ndarray) else x)\n",
    "\n",
    "# Insert data in batches\n",
    "batch_size = 10000\n",
    "for start in tqdm(range(0, len(df), batch_size)):\n",
    "    batch = df.iloc[start:start + batch_size]\n",
    "    \n",
    "    ids = batch['doc_id'].astype(str).tolist()\n",
    "    documents = batch['content'].tolist()\n",
    "    embeds = [embed.tolist() if isinstance(embed, np.ndarray) else embed for embed in batch['embeddings']]\n",
    "    metadatas = batch[['title', 'date', 'domain', 'url', 'language']].to_dict(orient='records')\n",
    "    \n",
    "    # Insert into ChromaDB collection\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=documents,\n",
    "        embeddings=embeds,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "\n",
    "print(\"Data successfully added to ChromaDB.\")"
   ],
   "id": "fa6e0104e17860ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_db = collection.get(include=['embeddings', 'documents', 'metadatas'], limit=1)\n",
    "print(test_db)"
   ],
   "id": "5673230f24901f0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Query similar documents\n",
    "question = 'In 2021, what were the top 3 states in the US in terms of total solar power generating capacity?'\n",
    "query_test = embeddings.embed_query(question)\n",
    "print(query_test[:3])"
   ],
   "id": "34c0174a507e00fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query_embedding = prepare_embedding_for_chromadb(query_test)\n",
    "top_k = 5  # number of similar entries to retrieve\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding.tolist()],\n",
    "    n_results=top_k,\n",
    "    include=['documents', 'metadatas']\n",
    ")\n",
    "\n",
    "print(results)"
   ],
   "id": "cfc075e2ef67caf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Retrieval",
   "id": "cb8ad6e258a0588f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T22:44:06.692894Z",
     "start_time": "2024-11-05T22:44:02.007846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ai_client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# load eval dataset\n",
    "df_eval = pd.read_csv('data/eval_dataset/cleantech_rag_evaluation_data_2024-02-23.csv')\n",
    "\n",
    "settings = Settings()\n",
    "\n",
    "client = chromadb.PersistentClient(path=storage_path, settings=settings)\n",
    "\n",
    "collection_name = \"energy_articles\"\n",
    "collection = client.get_collection(collection_name)\n",
    "\n",
    "eval_data_index = df_eval.sample(n=1)\n",
    "eval_question = eval_data_index.iloc[0]['question']\n",
    "article_url = eval_data_index.iloc[0]['article_url']\n",
    "\n",
    "# Query text\n",
    "query_text = eval_question\n",
    "\n",
    "# Generate query embedding using the Hugging Face endpoint\n",
    "#query_embedding = embeddings.embed_query(query_text) # remote\n",
    "query_embedding = embed_local.embed_query(query_text) # local\n",
    "\n",
    "prepared_embeddings = prepare_embedding_for_chromadb(query_embedding)\n",
    "\n",
    "# Retrieve top 5 most relevant documents\n",
    "results = collection.query(\n",
    "    query_embeddings=[prepared_embeddings.tolist()],  # Query embedding\n",
    "    n_results=5,  # Number of similar documents to retrieve\n",
    "    include=['documents', 'metadatas']  # Include documents and metadata in the results\n",
    ")\n",
    "\n",
    "#print(\"Query Results:\", results)\n",
    "\n",
    "# Prepare context with document references\n",
    "retrieved_text = \"\"\n",
    "if 'documents' in results and results['documents']:\n",
    "    for idx, doc in enumerate(results['documents'][0]):\n",
    "        metadata = results['metadatas'][0][idx]  # Access metadata for each document\n",
    "        doc_id = metadata.get(\"doc_id\", f\"Document {idx + 1}\")  # Retrieve doc_id if available\n",
    "        title = metadata.get(\"title\", \"Untitled Document\")\n",
    "        url = metadata.get(\"url\", \"URL not available\")\n",
    "        content_snippet = doc[:300] + \"...\"  # Take the first 300 characters as a snippet\n",
    "\n",
    "        retrieved_text += (\n",
    "            f\"Document {idx + 1} - ID: {doc_id}\\n\"\n",
    "            f\"Title: {title}\\n\"\n",
    "            f\"URL: {url}\\n\"\n",
    "            f\"Content Snippet: {content_snippet}\\n\\n\"\n",
    "        )\n",
    "else:\n",
    "    print(\"No documents found in query results.\")\n",
    "\n",
    "# Debug: Print the retrieved_text to ensure it’s populated\n",
    "#print(\"Retrieved Text:\", retrieved_text)\n",
    "# Create a system message with instructions for the assistant\n",
    "system_message = \"\"\"\n",
    "You are a knowledgeable assistant. Based on the information from the documents provided by the user, answer the question in a detailed and informative way. In your answer, refer to specific documents by mentioning their titles, URLs, and IDs when relevant.\n",
    "\n",
    "At the end of your answer, please provide a separate \"Sources\" section, listing all document titles, IDs, and URLs you referenced, even if they were only indirectly useful.\n",
    "\"\"\"\n",
    "\n",
    "# Construct the prompt as the user's message\n",
    "prompt = f\"\"\"\n",
    "Question: {query_text}\n",
    "\n",
    "Documents:\n",
    "{retrieved_text}\n",
    "\n",
    "Please structure your answer as follows:\n",
    "Answer:\n",
    "(Your detailed answer here, with references to specific documents as needed)\n",
    "\n",
    "Sources:\n",
    "- Document N: documnet_id document_title, document_url\n",
    "- Document N: documnet_id, document_title, document_url\n",
    "- Document N: documnet_id, document_title, document_url\n",
    "(Include every document you referred to in the answer)\n",
    "\"\"\"\n",
    "\n",
    "# Generate a response with GPT-3.5-turbo\n",
    "response = ai_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "generated_response = response.choices[0].message.content\n",
    "\n",
    "# Print the generated response\n",
    "print(f'Used question: {eval_question}\\nURL: {article_url}')\n",
    "print('-'*40)\n",
    "print(generated_response)\n",
    "print('-'*40)"
   ],
   "id": "f60c730af332e4cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used question: Who develops quality control systems for ocean temperature in-situ profiles?\n",
      "URL: https://www.azocleantech.com/news.aspx?newsID=32873\n",
      "----------------------------------------\n",
      "Answer:\n",
      "The quality control system for ocean temperature in-situ profiles is developed by scientists from the Chinese Academy of Sciences' (CAS) Institute of Atmospheric Physics (IAP) and their associates. This system, known as the CAS Ocean Data Center Quality Control system, offers a novel climatological range-based automatic quality control mechanism for ocean temperature in situ profiles. The quality controlled and bias-corrected ocean in-situ profile data from CAS Ocean Data Center, part of the Global Ocean Science Database, are now accessible to the public, ensuring data reliability and accuracy (Document 1, Document 3).\n",
      "\n",
      "The development of such quality control systems is crucial due to the vast amount of ocean temperature profiles collected over the last century, each with varying precision, quality, and metadata completeness. Ensuring the accuracy and reliability of this data is essential for climate research and modeling (Document 2).\n",
      "\n",
      "Overall, the CAS Ocean Data Center, in collaboration with the Global Ocean Science Database, plays a significant role in developing and implementing quality control systems for ocean temperature in-situ profiles, thereby enhancing the reliability and usability of oceanographic data for climate research and analysis.\n",
      "\n",
      "Sources:\n",
      "- Document 1: Quality Control System for Ocean Temperature In-Situ Profiles, https://www.azocleantech.com/news.aspx?newsID=32873\n",
      "- Document 2: Quality Control System for Ocean Temperature In-Situ Profiles, https://www.azocleantech.com/news.aspx?newsID=32873\n",
      "- Document 3: Quality Control System for Ocean Temperature In-Situ Profiles, https://www.azocleantech.com/news.aspx?newsID=32873\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:14:14.157315Z",
     "start_time": "2024-11-05T23:13:25.529312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import ContextPrecision, Faithfulness, AnswerRelevancy, ContextRecall\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# Initialize the LLM for metrics that require it\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "# Prepare the data with required columns\n",
    "data = [\n",
    "    {\n",
    "        \"question\": row[\"question\"],\n",
    "        \"user_input\": row[\"question\"],  # Same as the question\n",
    "        \"context\": [row[\"relevant_chunk\"]],  # Relevant context or chunk\n",
    "        \"retrieved_contexts\": [row[\"relevant_chunk\"]],  # Retrieved context(s)\n",
    "        \"response\": response.choices[0].message.content,  # Use actual response if available\n",
    "        \"reference\": \"Expected answer here\"  # Replace with ground truth if available\n",
    "    }\n",
    "    for _, row in df_eval.iterrows()\n",
    "]\n",
    "\n",
    "# Create the EvaluationDataset\n",
    "eval_dataset = EvaluationDataset.from_list(data)\n",
    "\n",
    "# Define metrics to use for evaluation\n",
    "metrics = [\n",
    "    ContextPrecision(),\n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    AnswerRelevancy(llm=evaluator_llm),\n",
    "    ContextRecall()\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "results = evaluate(eval_dataset, metrics=metrics)\n",
    "\n",
    "# Display the results\n",
    "print(\"Evaluation Results:\", results)"
   ],
   "id": "c9714fab2353c9af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Evaluating:   0%|          | 0/92 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c437cc3e38c54c76b52a81f4a6e494b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'context_precision': 1.0000, 'faithfulness': 0.0425, 'answer_relevancy': 0.7428, 'context_recall': 0.2572}\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T23:14:58.922027Z",
     "start_time": "2024-11-05T23:14:58.914058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_results = results.to_pandas()\n",
    "df_results.head()"
   ],
   "id": "3f77dfa9ef6fe2d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is the innovation behind Leclanché's new ...   \n",
       "1       What is the EU’s Green Deal Industrial Plan?   \n",
       "2       What is the EU’s Green Deal Industrial Plan?   \n",
       "3  What are the four focus areas of the EU's Gree...   \n",
       "4  When did the cooperation between GM and Honda ...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Leclanché said it has developed an environmen...   \n",
       "1  [The Green Deal Industrial Plan is a bid by th...   \n",
       "2  [The European counterpart to the US Inflation ...   \n",
       "3  [The new plan is fundamentally focused on four...   \n",
       "4  [What caught our eye was a new hookup between ...   \n",
       "\n",
       "                                            response             reference  \\\n",
       "0  Answer:\\nThe quality control system for ocean ...  Expected answer here   \n",
       "1  Answer:\\nThe quality control system for ocean ...  Expected answer here   \n",
       "2  Answer:\\nThe quality control system for ocean ...  Expected answer here   \n",
       "3  Answer:\\nThe quality control system for ocean ...  Expected answer here   \n",
       "4  Answer:\\nThe quality control system for ocean ...  Expected answer here   \n",
       "\n",
       "   context_precision  faithfulness  answer_relevancy  context_recall  \n",
       "0                1.0           0.0          0.741779            0.75  \n",
       "1                1.0           0.0          0.732744            1.00  \n",
       "2                1.0           0.0          0.732744            0.00  \n",
       "3                1.0           0.0          0.730669            0.50  \n",
       "4                1.0           0.0          0.752416            0.00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the innovation behind Leclanché's new ...</td>\n",
       "      <td>[Leclanché said it has developed an environmen...</td>\n",
       "      <td>Answer:\\nThe quality control system for ocean ...</td>\n",
       "      <td>Expected answer here</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.741779</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>[The Green Deal Industrial Plan is a bid by th...</td>\n",
       "      <td>Answer:\\nThe quality control system for ocean ...</td>\n",
       "      <td>Expected answer here</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732744</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the EU’s Green Deal Industrial Plan?</td>\n",
       "      <td>[The European counterpart to the US Inflation ...</td>\n",
       "      <td>Answer:\\nThe quality control system for ocean ...</td>\n",
       "      <td>Expected answer here</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.732744</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the four focus areas of the EU's Gree...</td>\n",
       "      <td>[The new plan is fundamentally focused on four...</td>\n",
       "      <td>Answer:\\nThe quality control system for ocean ...</td>\n",
       "      <td>Expected answer here</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.730669</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When did the cooperation between GM and Honda ...</td>\n",
       "      <td>[What caught our eye was a new hookup between ...</td>\n",
       "      <td>Answer:\\nThe quality control system for ocean ...</td>\n",
       "      <td>Expected answer here</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.752416</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "809a119d9596a014"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
